{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"chetanmittal033/bank-dataset-for-customer-churn-prediction\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XbNnXRGzM26",
        "outputId": "71e1f66a-64fe-4ceb-f6b8-5969fcaae42e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.13), please consider upgrading to the latest version (0.4.3).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/chetanmittal033/bank-dataset-for-customer-churn-prediction?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 262k/262k [00:00<00:00, 16.4MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n",
            "Path to dataset files: /root/.cache/kagglehub/datasets/chetanmittal033/bank-dataset-for-customer-churn-prediction/versions/1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opendatasets\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hGs1hGTR-fF",
        "outputId": "10801ddf-c39d-487e-c083-4a12638a197c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opendatasets\n",
            "  Downloading opendatasets-0.1.22-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from opendatasets) (4.67.3)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.12/dist-packages (from opendatasets) (1.7.4.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from opendatasets) (8.3.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (6.3.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (2026.1.4)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (3.4.4)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (3.11)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (5.29.6)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (0.5.1)\n",
            "Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: opendatasets\n",
            "Successfully installed opendatasets-0.1.22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import opendatasets as od\n",
        "od.download(\"https://www.kaggle.com/datasets/chetanmittal033/bank-dataset-for-customer-churn-prediction\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hxSXxGBSba4",
        "outputId": "9acee218-e740-4782-d956-11e1ba12dc9b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: shubhangiagarwal0\n",
            "Your Kaggle Key: ··········\n",
            "Dataset URL: https://www.kaggle.com/datasets/chetanmittal033/bank-dataset-for-customer-churn-prediction\n",
            "Downloading bank-dataset-for-customer-churn-prediction.zip to ./bank-dataset-for-customer-churn-prediction\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 262k/262k [00:00<00:00, 346MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAaGJe5uyd7c",
        "outputId": "99826bac-3c1b-430e-e31a-4fbe86f256b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Confusion Matrix for Logistic Regression:\n",
            "[[1552   41]\n",
            " [ 349   58]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.97      0.89      1593\n",
            "           1       0.59      0.14      0.23       407\n",
            "\n",
            "    accuracy                           0.81      2000\n",
            "   macro avg       0.70      0.56      0.56      2000\n",
            "weighted avg       0.77      0.81      0.75      2000\n",
            "\n",
            "\n",
            "Confusion Matrix for Decision Tree:\n",
            "[[1357  236]\n",
            " [ 213  194]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.85      0.86      1593\n",
            "           1       0.45      0.48      0.46       407\n",
            "\n",
            "    accuracy                           0.78      2000\n",
            "   macro avg       0.66      0.66      0.66      2000\n",
            "weighted avg       0.78      0.78      0.78      2000\n",
            "\n",
            "\n",
            "Confusion Matrix for KNN:\n",
            "[[1513   80]\n",
            " [ 250  157]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.95      0.90      1593\n",
            "           1       0.66      0.39      0.49       407\n",
            "\n",
            "    accuracy                           0.83      2000\n",
            "   macro avg       0.76      0.67      0.69      2000\n",
            "weighted avg       0.82      0.83      0.82      2000\n",
            "\n",
            "\n",
            "Confusion Matrix for Naive Bayes:\n",
            "[[1562   31]\n",
            " [ 311   96]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.98      0.90      1593\n",
            "           1       0.76      0.24      0.36       407\n",
            "\n",
            "    accuracy                           0.83      2000\n",
            "   macro avg       0.79      0.61      0.63      2000\n",
            "weighted avg       0.82      0.83      0.79      2000\n",
            "\n",
            "\n",
            "Confusion Matrix for Random Forest:\n",
            "[[1542   51]\n",
            " [ 220  187]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.97      0.92      1593\n",
            "           1       0.79      0.46      0.58       407\n",
            "\n",
            "    accuracy                           0.86      2000\n",
            "   macro avg       0.83      0.71      0.75      2000\n",
            "weighted avg       0.86      0.86      0.85      2000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [07:56:02] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Confusion Matrix for XGBoost:\n",
            "[[1502   91]\n",
            " [ 215  192]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.94      0.91      1593\n",
            "           1       0.68      0.47      0.56       407\n",
            "\n",
            "    accuracy                           0.85      2000\n",
            "   macro avg       0.78      0.71      0.73      2000\n",
            "weighted avg       0.83      0.85      0.84      2000\n",
            "\n",
            "\n",
            "Model Performance Comparison:\n",
            "\n",
            "                     Accuracy     AUC  Precision  Recall  F1 Score     MCC\n",
            "Logistic Regression    0.8050  0.7710     0.5859  0.1425    0.2292  0.2167\n",
            "Decision Tree          0.7755  0.6643     0.4512  0.4767    0.4636  0.3219\n",
            "KNN                    0.8350  0.7724     0.6624  0.3857    0.4876  0.4180\n",
            "Naive Bayes            0.8290  0.8146     0.7559  0.2359    0.3596  0.3573\n",
            "Random Forest          0.8645  0.8469     0.7857  0.4595    0.5798  0.5315\n",
            "XGBoost                0.8470  0.8330     0.6784  0.4717    0.5565  0.4789\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, roc_auc_score, precision_score,\n",
        "    recall_score, f1_score, matthews_corrcoef\n",
        ")\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "import joblib\n",
        "import os\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# ===============================\n",
        "# Load Dataset\n",
        "# ===============================\n",
        "df = pd.read_csv(\"/content/bank-dataset-for-customer-churn-prediction/Churn_Modelling.csv\")   # <-- update filename if needed\n",
        "\n",
        "# ===============================\n",
        "# Drop unnecessary columns\n",
        "# ===============================\n",
        "df.drop(columns=[\"RowNumber\", \"CustomerId\", \"Surname\"], inplace=True)\n",
        "\n",
        "# ===============================\n",
        "# Encode categorical features\n",
        "# ===============================\n",
        "label_encoder = LabelEncoder()\n",
        "df[\"Gender\"] = label_encoder.fit_transform(df[\"Gender\"])\n",
        "df[\"Geography\"] = label_encoder.fit_transform(df[\"Geography\"])\n",
        "\n",
        "# ===============================\n",
        "# Split features and target\n",
        "# ===============================\n",
        "X = df.drop(\"Exited\", axis=1)\n",
        "y = df[\"Exited\"]\n",
        "\n",
        "# ===============================\n",
        "# Train-Test Split\n",
        "# ===============================\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# ===============================\n",
        "# Feature Scaling\n",
        "# ===============================\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Save scaler for Streamlit\n",
        "os.makedirs(\"model\", exist_ok=True)\n",
        "joblib.dump(scaler, \"model/scaler.pkl\")\n",
        "\n",
        "# ===============================\n",
        "# Models\n",
        "# ===============================\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
        "    \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
        "    \"Naive Bayes\": GaussianNB(),\n",
        "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    \"XGBoost\": XGBClassifier(\n",
        "        use_label_encoder=False,\n",
        "        eval_metric=\"logloss\",\n",
        "        random_state=42\n",
        "    )\n",
        "}\n",
        "\n",
        "# ===============================\n",
        "# Evaluation Function\n",
        "# ===============================\n",
        "def evaluate_model(model, X_test, y_test, model_name):\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_prob = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # Metrics\n",
        "    metrics = {\n",
        "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"AUC\": roc_auc_score(y_test, y_prob),\n",
        "        \"Precision\": precision_score(y_test, y_pred),\n",
        "        \"Recall\": recall_score(y_test, y_pred),\n",
        "        \"F1 Score\": f1_score(y_test, y_pred),\n",
        "        \"MCC\": matthews_corrcoef(y_test, y_pred)\n",
        "    }\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    print(f\"\\nConfusion Matrix for {model_name}:\")\n",
        "    print(cm)\n",
        "\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# Train, Evaluate, Save Models\n",
        "# ===============================\n",
        "results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    metrics = evaluate_model(model, X_test, y_test,name)\n",
        "    results[name] = metrics\n",
        "\n",
        "    # Save model\n",
        "    filename = name.lower().replace(\" \", \"_\") + \".pkl\"\n",
        "    joblib.dump(model, f\"model/{filename}\")\n",
        "\n",
        "# ===============================\n",
        "# Display Results\n",
        "# ===============================\n",
        "results_df = pd.DataFrame(results).T\n",
        "print(\"\\nModel Performance Comparison:\\n\")\n",
        "print(results_df.round(4))\n"
      ]
    }
  ]
}